{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef60ab70-24bd-4a34-a76b-d05dab4a2c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import shutil\n",
    "import random\n",
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import time\n",
    "import cupy as cp\n",
    "from multiprocessing import Pool\n",
    "import argparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95654d0-06b6-435c-afc1-4198d3e5d20d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795ab366-97c6-4547-afa4-84a8322ccbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def alphanum_key(s):\n",
    "    \"\"\" Turn a string into a list of string and number chunks.\n",
    "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
    "    \"\"\"\n",
    "    return [int(c) if c.isdigit() else c for c in re.split('([0-9]+)',s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f807b823-0ebd-43a3-8645-64d9fdcb8aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_new_hdf5_file(filename, max_rows_per_file):\n",
    "    #filename = f\"{prefix}_{index}.h5\"\n",
    "    hdf5_file = h5py.File(filename, 'w')\n",
    "    dataset_names = ['all_jet', 'am', 'ieta', 'iphi', 'm0']\n",
    "    total_samples = max_rows_per_file\n",
    "    datasets = {\n",
    "        name: hdf5_file.create_dataset(\n",
    "        name,\n",
    "        (total_samples, 13, 125, 125) if 'jet' in name else (total_samples, 1),\n",
    "        dtype='float32',  # Specify an appropriate data type\n",
    "        compression='lzf',  # Optional: add compression\n",
    "        #chunks = (min_samples, 13, 125, 125) if 'jet' in name else (min_samples, 1),\n",
    "        ) for name in dataset_names\n",
    "    }\n",
    "    #hdf5_file.create_dataset('dataset', shape=(0, *data.shape[1:]), maxshape=(max_rows_per_file, *data.shape[1:]), dtype='float64')  # Adjust dtype as per your data type\n",
    "    return hdf5_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b0ee0b-8556-4f56-90d1-bb27dcdb00c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_data_to_hdf5(hdf5_file, start_index, end_index, df):\n",
    "    #df = df[start_index:end_index]\n",
    "    \n",
    "    print(\"Writing to file\", hdf5_file)\n",
    "    xj = df.columns.get_loc('X_jet')\n",
    "    am = df.columns.get_loc('am')\n",
    "    ieta = df.columns.get_loc('ieta')\n",
    "    iphi = df.columns.get_loc('iphi')\n",
    "    m0 = df.columns.get_loc('m0')\n",
    "\n",
    "    im = np.array(np.array(np.array(df.iloc[:, xj].tolist()).tolist()).tolist())\n",
    "    am = np.array(df.iloc[:,am])\n",
    "    # print(\"meta --- \", meta)\n",
    "    ieta = np.array(df.iloc[:,ieta])\n",
    "    iphi = np.array(df.iloc[:,iphi])\n",
    "    m0 = np.array(df.iloc[:,m0])\n",
    "    \n",
    "#     print(\"In Append function dataframe shape----\", df.shape)\n",
    "#     print(\"shape hdf5----\", hdf5_file[\"all_jet\"][start_index:end_index, :, :, :].shape)\n",
    "#     print(\"jet image shape----\", im.shape)\n",
    "#     print(\"am ----\", am)\n",
    "#     print(\"ieta ----\", ieta)\n",
    "#     print(\"iphi ----\", iphi)\n",
    "#     print(\"m0 ----\", m0)\n",
    "#     print(\"df.shape[0] ----- \",df.shape[0])\n",
    "    \n",
    "#     print(\"start_index--- \", start_index)\n",
    "#     print(\"end_index--- \", end_index)\n",
    "    hdf5_file[\"all_jet\"][start_index:end_index, :, :, :] = im\n",
    "    # print(\"np.full((df.shape[0],1), am)      \",np.full((df.shape[0],1), am.reshape(df.shape[0],1).tolist()))\n",
    "    # hdf5_file[\"am\"][start_index:end_index, :]   = np.full((df.shape[0],1), am.reshape(df.shape[0],1).tolist())\n",
    "    hdf5_file[\"am\"][start_index:end_index, :]   = am.reshape(df.shape[0],1).tolist()\n",
    "    hdf5_file[\"ieta\"][start_index:end_index, :] = ieta.reshape(df.shape[0],1).tolist()\n",
    "    hdf5_file[\"iphi\"][start_index:end_index, :] = iphi.reshape(df.shape[0],1).tolist()\n",
    "    hdf5_file[\"m0\"][start_index:end_index, :]   = m0.reshape(df.shape[0],1).tolist()\n",
    "    \n",
    "    return hdf5_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26b1ede-5d92-48b7-b9f0-197a87ff4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11cc8172-1e81-4c46-b625-01edc6222054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_files(args):\n",
    "#def process_files(file_path,h5py_file):\n",
    "    \n",
    "    file_path = args[0]\n",
    "    h5py_file = args[1]\n",
    "    batch_size = 6096\n",
    "    # batch_size = 10\n",
    "    \n",
    "    print(\"------Processing file------\")\n",
    "    parquet = pq.ParquetFile(file_path)\n",
    "    print(\"file ------>   \", file_path)\n",
    "    total = parquet.num_row_groups\n",
    "    print(\"Number of row --------> \", total)\n",
    "    total_samples = parquet.num_row_groups\n",
    "    hdf5_file = create_new_hdf5_file(h5py_file,total_samples)\n",
    "    batch_iter = parquet.iter_batches(batch_size,use_threads=True)\n",
    "\n",
    "    start_index = 0\n",
    "    bat = 0\n",
    "    for batch in batch_iter:\n",
    "        #batch = next(batch_iter)\n",
    "        df = batch.to_pandas(use_threads=True)\n",
    "        end_index = start_index + df.shape[0]\n",
    "        print(\"total----->\",total , \" Batch no.\", bat, \"Data frame shape\", df.shape, \" Start idx:\", start_index, \" end idx:\", end_index)\n",
    "\n",
    "        if end_index<=total_samples:\n",
    "            #print(\"Image shape going in append\", im.shape, \" \", start_index, \" \", end_index)\n",
    "            append_data_to_hdf5(hdf5_file, start_index, end_index, df)\n",
    "            start_index += df.shape[0]\n",
    "            # break\n",
    "\n",
    "        bat +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ca1e8-ccbb-443d-980d-e722e86529a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40628bb-e9e4-4a0f-b37a-21a0020a76eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # parquet_dir = '/pscratch/sd/r/rchudasa/E2E_samples/ParquetFiles_correctTrackerLayerHits_SecVtxInfoAdded/'\n",
    "# parquet_dir = '/pscratch/sd/b/bbbam/'\n",
    "# h5_dir = '/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_train_hd5/'\n",
    "# if not os.path.exists(h5_dir):\n",
    "#     # Create the directory if it doesn't exist\n",
    "#     os.makedirs(h5_dir)\n",
    "# h5_name = 'tau_threads.h5'\n",
    "# batch_size = 4096\n",
    "# # batch_size = 10\n",
    "# # signal_files = [os.path.join(parquet_dir + 'signal/', f) for f in os.listdir(parquet_dir + 'signal/')]\n",
    "# # bkg_files = [os.path.join(parquet_dir + 'background/', f) for f in os.listdir(parquet_dir + 'background/')]\n",
    "\n",
    "# signal_files = [os.path.join(parquet_dir + 'IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_all/', f) for f in os.listdir(parquet_dir + 'IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_all/')]\n",
    "\n",
    "# combined_files = signal_files\n",
    "# # random.shuffle(combined_files)\n",
    "# # print(\"type(combined_files)  :-----\", type(combined_files))\n",
    "\n",
    "# counter = 0\n",
    "\n",
    "# inputfile_list = []\n",
    "# outputfile_list = []\n",
    "\n",
    "# for f in combined_files:\n",
    "#     opFile       = f.split(\"/\")[-1].split(\".\")[0]\n",
    "#     # proceessName = opFile.split(\"_\")[0]\n",
    "#     # processID    = opFile.split(\"_\")[-1]\n",
    "#     # print(\"opFile-----\", opFile)\n",
    "#     h5_file = h5_dir+opFile+\".h5\"\n",
    "#     # print(\"h5_file----\", h5_file)\n",
    "#     #process_files(f, h5_file, batch_size)\n",
    "#     inputfile_list.append(f)\n",
    "#     outputfile_list.append(h5_file)\n",
    "#     tic = time.time()\n",
    "#     #tic = time.time()\n",
    "#     #process_files(f,h5_file)\n",
    "#     #toc = time.time()\n",
    "#     #print(\"It took {} minutes to run {} file\".format((toc-tic)/60,f))\n",
    "#     #counter =+ 1\n",
    "#     #if counter >=10:\n",
    "#     #break\n",
    "\n",
    "# #print(inputfile_list[0:10])\n",
    "# #print(outputfile_list[0:10])\n",
    "# #args = list(zip(inputfile_list[0:10],outputfile_list[0:10]))\n",
    "# args = list(zip(inputfile_list,outputfile_list)) \n",
    "# print(\"----------------------------------------\")\n",
    "# print(\"arg --------\", args)\n",
    "\n",
    "# with Pool(12) as p:\n",
    "#     # print(\"**************\",args)\n",
    "#     p.map(process_files,args)\n",
    "# toc = time.time()\n",
    "\n",
    "\n",
    "# print(\"It took \", toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f302ba-2b26-483f-906c-19debbf1dc92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# h5_dir = '/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To3p6_dataset_2_unbaised_v2_train_hd5/'\n",
    "# if not os.path.exists(h5_dir):\n",
    "#     # Create the directory if it doesn't exist\n",
    "#     os.makedirs(h5_dir)\n",
    "# h5_name = 'tau_threads.h5'\n",
    "# batch_size = 4096\n",
    "\n",
    "\n",
    "# parquet_files = glob.glob(\"/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To3p6_m14p8To17p2_dataset_2_unbaised_v2_train/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To3p6_dataset_2_unbaised_unphysical_0009*\")\n",
    "\n",
    "# inputfile_list = []\n",
    "# outputfile_list = []\n",
    "\n",
    "# for f in parquet_files:\n",
    "#     opFile       = f.split(\"/\")[-1].split(\".\")[0]\n",
    "#     # proceessName = opFile.split(\"_\")[0]\n",
    "#     # processID    = opFile.split(\"_\")[-1]\n",
    "#     # print(\"opFile-----\", opFile)\n",
    "#     h5_file = h5_dir+opFile+\".h5\"\n",
    "#     # print(\"h5_file----\", h5_file)\n",
    "#     #process_files(f, h5_file, batch_size)\n",
    "#     inputfile_list.append(f)\n",
    "#     outputfile_list.append(h5_file)\n",
    "#     tic = time.time()\n",
    "#     #tic = time.time()\n",
    "#     #process_files(f,h5_file)\n",
    "#     #toc = time.time()\n",
    "#     #print(\"It took {} minutes to run {} file\".format((toc-tic)/60,f))\n",
    "#     #counter =+ 1\n",
    "#     #if counter >=10:\n",
    "#     #break\n",
    "\n",
    "# #print(inputfile_list[0:10])\n",
    "# #print(outputfile_list[0:10])\n",
    "# #args = list(zip(inputfile_list[0:10],outputfile_list[0:10]))\n",
    "# args = list(zip(inputfile_list,outputfile_list)) \n",
    "# print(\"----------------------------------------\")\n",
    "# print(\"arg --------\", args)\n",
    "\n",
    "# with Pool(len(parquet_files)) as p:\n",
    "#     # print(\"**************\",args)\n",
    "#     p.map(process_files,args)\n",
    "# toc = time.time()\n",
    "\n",
    "\n",
    "# print(\"It took \", toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61e321-afe0-4278-9fe3-d24328c8da95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "arg -------- [('/pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M8_signal_v2.parquet', '/pscratch/sd/b/bbbam/signal_hd5/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M8_signal_v2.h5'), ('/pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.parquet', '/pscratch/sd/b/bbbam/signal_hd5/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5'), ('/pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M14_signal_v2.parquet', '/pscratch/sd/b/bbbam/signal_hd5/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M14_signal_v2.h5'), ('/pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M3p7_signal_v2_1.parquet', '/pscratch/sd/b/bbbam/signal_hd5/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M3p7_signal_v2_1.h5'), ('/pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M5_signal_v2_1.parquet', '/pscratch/sd/b/bbbam/signal_hd5/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M5_signal_v2_1.h5'), ('/pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M10_signal_v2.parquet', '/pscratch/sd/b/bbbam/signal_hd5/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M10_signal_v2.h5'), ('/pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M12_signal_v2.parquet', '/pscratch/sd/b/bbbam/signal_hd5/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M12_signal_v2.h5'), ('/pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M6_signal_v2.parquet', '/pscratch/sd/b/bbbam/signal_hd5/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M6_signal_v2.h5')]\n",
      "------Processing file------------Processing file------------Processing file------------Processing file------------Processing file------------Processing file------------Processing file------------Processing file------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "file ------>    /pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M14_signal_v2.parquet\n",
      "Number of row -------->  24718\n",
      "file ------>    /pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M12_signal_v2.parquet\n",
      "Number of row -------->  65255\n",
      "file ------>    /pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M10_signal_v2.parquet\n",
      "Number of row -------->  279331\n",
      "file ------>    /pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M5_signal_v2_1.parquet\n",
      "Number of row -------->  519319\n",
      "file ------>    /pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M8_signal_v2.parquet\n",
      "Number of row -------->  544426\n",
      "file ------>    /pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.parquet\n",
      "Number of row -------->  677934\n",
      "file ------>    /pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M3p7_signal_v2_1.parquet\n",
      "Number of row -------->  708412\n",
      "file ------>    /pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M6_signal_v2.parquet\n",
      "Number of row -------->  792604\n",
      "total-----> 708412  Batch no. 0 Data frame shape (6096, 8)  Start idx: 0  end idx: 6096\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M3p7_signal_v2_1.h5\" (mode r+)>\n",
      "total-----> 677934  Batch no. 0 Data frame shape (6096, 8)  Start idx: 0  end idx: 6096\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5\" (mode r+)>\n",
      "total-----> 677934  Batch no. 1 Data frame shape (6096, 8)  Start idx: 6096  end idx: 12192\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5\" (mode r+)>\n",
      "total-----> 677934  Batch no. 2 Data frame shape (6096, 8)  Start idx: 12192  end idx: 18288\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5\" (mode r+)>\n",
      "total-----> 677934  Batch no. 3 Data frame shape (6096, 8)  Start idx: 18288  end idx: 24384\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5\" (mode r+)>\n",
      "total-----> 677934  Batch no. 4 Data frame shape (6096, 8)  Start idx: 24384  end idx: 30480\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5\" (mode r+)>\n",
      "total-----> 677934  Batch no. 5 Data frame shape (6096, 8)  Start idx: 30480  end idx: 36576\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5\" (mode r+)>\n",
      "total-----> 677934  Batch no. 6 Data frame shape (6096, 8)  Start idx: 36576  end idx: 42672\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5\" (mode r+)>\n",
      "total-----> 677934  Batch no. 7 Data frame shape (6096, 8)  Start idx: 42672  end idx: 48768\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5\" (mode r+)>\n",
      "total-----> 677934  Batch no. 8 Data frame shape (6096, 8)  Start idx: 48768  end idx: 54864\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5\" (mode r+)>\n",
      "total-----> 677934  Batch no. 9 Data frame shape (6096, 8)  Start idx: 54864  end idx: 60960\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5\" (mode r+)>\n",
      "total-----> 677934  Batch no. 10 Data frame shape (6096, 8)  Start idx: 60960  end idx: 67056\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5\" (mode r+)>\n",
      "total-----> 677934  Batch no. 11 Data frame shape (6096, 8)  Start idx: 67056  end idx: 73152\n",
      "Writing to file <HDF5 file \"IMG_H_AATo4Tau_Hadronic_tauDR0p4_M4_signal_v2_1.h5\" (mode r+)>\n"
     ]
    }
   ],
   "source": [
    "h5_dir = '/pscratch/sd/b/bbbam/signal_hd5/'\n",
    "if not os.path.exists(h5_dir):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(h5_dir)\n",
    "\n",
    "batch_size = 4096\n",
    "\n",
    "\n",
    "parquet_files = glob.glob(\"/pscratch/sd/b/bbbam/signal/*\")\n",
    "\n",
    "inputfile_list = []\n",
    "outputfile_list = []\n",
    "\n",
    "for f in parquet_files:\n",
    "    opFile       = f.split(\"/\")[-1].split(\".\")[0]\n",
    "    # proceessName = opFile.split(\"_\")[0]\n",
    "    # processID    = opFile.split(\"_\")[-1]\n",
    "    # print(\"opFile-----\", opFile)\n",
    "    h5_file = h5_dir+opFile+\".h5\"\n",
    "    # print(\"h5_file----\", h5_file)\n",
    "    #process_files(f, h5_file, batch_size)\n",
    "    inputfile_list.append(f)\n",
    "    outputfile_list.append(h5_file)\n",
    "    tic = time.time()\n",
    "    #tic = time.time()\n",
    "    #process_files(f,h5_file)\n",
    "    #toc = time.time()\n",
    "    #print(\"It took {} minutes to run {} file\".format((toc-tic)/60,f))\n",
    "    #counter =+ 1\n",
    "    #if counter >=10:\n",
    "    #break\n",
    "\n",
    "#print(inputfile_list[0:10])\n",
    "#print(outputfile_list[0:10])\n",
    "#args = list(zip(inputfile_list[0:10],outputfile_list[0:10]))\n",
    "args = list(zip(inputfile_list,outputfile_list)) \n",
    "print(\"----------------------------------------\")\n",
    "print(\"arg --------\", args)\n",
    "\n",
    "with Pool(len(parquet_files)) as p:\n",
    "    # print(\"**************\",args)\n",
    "    p.map(process_files,args)\n",
    "toc = time.time()\n",
    "\n",
    "\n",
    "print(\"It took \", toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8c94742-8fd3-45bd-b2fe-4a9853d11f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pscratch/sd/b/bbbam/signal/IMG_H_AATo4Tau_Hadronic_tauDR0p4_M5_signal_v2_1.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IMG_H_AATo4Tau_Hadronic_tauDR0p4_M5_signal_v2_1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae3edc-4586-4487-92cc-a965a605d106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
