{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06bcfec0-d9e3-4812-aae0-902290409b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "run = 0\n",
    "np.random.seed(run)\n",
    "import os, glob\n",
    "import time\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import mplhep as hep\n",
    "plt.style.use([hep.style.ROOT, hep.style.firamath])\n",
    "#from skimage.transform import rescale\n",
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "from torch.utils.data import *\n",
    "import pandas as pd\n",
    "import pickle\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e98eb5-c5b3-47f5-a59a-52ba110141dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873707a3-2586-49d0-bef7-035ff8d9d6cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if use_cuda:\n",
    "    device_ids = [0, 1, 2, 3]  # IDs of the 4 GPUs\n",
    "else:\n",
    "    device_ids = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3309f694-ec11-4efd-98e7-22b95392dac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82209ac9-c6f4-45d6-88dc-408107cf82fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0de27d3e-47cd-499e-a6e2-06bf8539b148",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A100-SXM4-40GB'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "851fbcad-007b-4d6c-b9ab-cb0b9befb098",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "786d9038-a4f6-4f24-ae86-82c13610dade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mass_trainer_aToTauTau_m1p2To17p2_v2_13ch_tets_only.torch_resnet_concat as networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63e2e941-1294-40ae-8b2c-98319572977d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def transform_y(y):\n",
    "#     return y/m0_scale\n",
    "\n",
    "# def inv_transform(y):\n",
    "#     return y*m0_scale\n",
    "\n",
    "# class ParquetDataset(Dataset):\n",
    "#     def __init__(self, filename, label):\n",
    "#         self.parquet = pq.ParquetFile(filename)\n",
    "#         #self.cols = None # read all columns\n",
    "#         #self.cols = ['X_jet.list.item.list.item.list.item','am','apt','iphi','ieta']\n",
    "#         self.cols = ['X_jet.list.item.list.item.list.item','am','iphi','ieta']\n",
    "#         self.label = label\n",
    "#     def __getitem__(self, index):\n",
    "#         data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "#         data['X_jet'] = np.float32(data['X_jet'][0])\n",
    "#         data['X_jet'][0] = pt_scale   * data['X_jet'][0] #Track pT\n",
    "#         data['X_jet'][1] = dz_scale   * data['X_jet'][1] #Track dZ\n",
    "#         data['X_jet'][2] = dz_scale   * data['X_jet'][2] #Track d0\n",
    "#         data['X_jet'][3] = ecal_scale * data['X_jet'][3] #ECAL\n",
    "#         data['X_jet'][4] = hcal_scale * data['X_jet'][4] #HCAL\n",
    "#         #data['X_jet'] = np.float32(data['X_jet'][0])/ecal_scale\n",
    "#         data['am'] = transform_y(np.float32(data['am']))\n",
    "#         #data['apt'] = np.float32(data['apt'])\n",
    "#         data['iphi'] = np.float32(data['iphi'])/360.\n",
    "#         data['ieta'] = np.float32(data['ieta'])/140.\n",
    "#         data['label'] = self.label\n",
    "#         # Preprocessing\n",
    "#         #data_dict['X_jet'] = data_dict['X_jet'][:, 20:105, 20:105]\n",
    "#         # High Value Suppressuib\n",
    "#         data['X_jet'][1][data['X_jet'][1] < -20] = 0\n",
    "#         data['X_jet'][1][data['X_jet'][1] >  20] = 0\n",
    "#         data['X_jet'][2][data['X_jet'][2] < -10] = 0\n",
    "#         data['X_jet'][2][data['X_jet'][2] >  10] = 0\n",
    "#         # Zero-Suppression\n",
    "#         data['X_jet'][0][data['X_jet'][0] < 1.e-3] = 0.\n",
    "#         # data['X_jet'][1][data['X_jet'][1] < 1.e-4] = 0.\n",
    "#         # data['X_jet'][2][data['X_jet'][2] < 1.e-4] = 0.\n",
    "#         data['X_jet'][3][data['X_jet'][3] < 1.e-3] = 0.\n",
    "#         data['X_jet'][4][data['X_jet'][4] < 1.e-3] = 0.\n",
    "\n",
    "#         indices = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "#         newdata = [data['X_jet'][index,:,:] for index in indices]\n",
    "#         data['X_jet'] = np.reshape(newdata, (len(indices),125,125))\n",
    "#         return dict(data)\n",
    "#     def __len__(self):\n",
    "#         return self.parquet.num_row_groups\n",
    "    \n",
    "# def mae_loss_wgtd(pred, true, wgt=1.):\n",
    "#     loss = wgt*(pred-true).abs().to(device)\n",
    "#     return loss.mean()    \n",
    "\n",
    "# def do_eval(resnet, val_loader, mae_best, epoch, sample):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     global expt_name\n",
    "#     loss_ = 0.\n",
    "#     m_pred_, m_true_, mae_, mre_ = [], [], [], []\n",
    "#     # iphi_, ieta_ = [], []\n",
    "#     now = time.time()\n",
    "#     ma_low = transform_y(3.6) # convert from GeV to network units\n",
    "#     for i, data in enumerate(val_loader):\n",
    "#         X, am = data['X_jet'].to(device), data['am'].to(device)\n",
    "#         iphi, ieta = data['iphi'].to(device), data['ieta'].to(device)\n",
    "#         logits = resnet([X, iphi, ieta])\n",
    "#         loss_ += mae_loss_wgtd(logits, am).item()\n",
    "#         logits, am = inv_transform(logits), inv_transform(am)\n",
    "#         mae = (logits-am).abs()\n",
    "#         mre = (((logits-am).abs())/am)\n",
    "\n",
    "#         if i % 100 == 0:\n",
    "\n",
    "#             print('Validation (%d/%d): Train loss:%f, mae:%f, mre:%f'%(i, len(val_loader), loss_/(i+1), mae.mean().item(), mre.mean().item() ))\n",
    "#         # Store batch metrics:\n",
    "#         m_pred_.append(logits.tolist())\n",
    "#         m_true_.append(am.tolist())\n",
    "#         mae_.append(mae.tolist())\n",
    "#         mre_.append(mre.tolist())\n",
    "        \n",
    "#         del logits\n",
    "#         gc.collect()\n",
    "#     now = time.time() - now\n",
    "#     m_true_ = np.concatenate(m_true_)\n",
    "#     m_pred_ = np.concatenate(m_pred_)\n",
    "#     mae_    = np.concatenate(mae_)\n",
    "#     mre_    = np.concatenate(mre_)\n",
    "#     score_str = 'epoch%d_%s_mae%.4f'%(epoch, sample, np.mean(mae_))\n",
    "#     lr_scheduler.step(loss_/len(val_loader))\n",
    "#     print(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "#     gc.collect()\n",
    "\n",
    "#     mae_retun = np.mean(mae_)\n",
    "#     return mae_retun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66971a9e-82a4-4296-886c-e0a4520861be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_y(y):\n",
    "    return y/m0_scale\n",
    "\n",
    "def inv_transform(y):\n",
    "    return y*m0_scale\n",
    "\n",
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, filename, label):\n",
    "        self.parquet = pq.ParquetFile(filename)\n",
    "        #self.cols = None # read all columns\n",
    "        #self.cols = ['X_jet.list.item.list.item.list.item','am','apt','iphi','ieta']\n",
    "        self.cols = ['X_jet.list.item.list.item.list.item','am','iphi','ieta']\n",
    "        self.label = label\n",
    "    def __getitem__(self, index):\n",
    "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "        data['X_jet'] = torch.tensor(data['X_jet'][0], dtype=torch.float32)\n",
    "        data['am'] = torch.tensor(transform_y(np.float32(data['am'])), dtype=torch.float32)\n",
    "        data['iphi'] = torch.tensor(np.float32(data['iphi'])/360., dtype=torch.float32)\n",
    "        data['ieta'] = torch.tensor(np.float32(data['ieta'])/140, dtype=torch.float32)\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        return self.parquet.num_row_groups\n",
    "    def __len__(self):\n",
    "        return self.parquet.num_row_groups\n",
    "    \n",
    "def mae_loss_wgtd(pred, true, wgt=1.):\n",
    "    loss = wgt*(pred-true).abs().to(device)\n",
    "    return loss.mean()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8f9b1-33b5-4c2c-8e88-f5dcf93997b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d413a7e0-3f57-4a40-8706-3515ccd33f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_init = 5.e-4\n",
    "lr_factor = 0.2\n",
    "new_lr = 0\n",
    "patience = 2\n",
    "resblocks = 3\n",
    "epochs = 2\n",
    "load_epoch = 0\n",
    "\n",
    "hcal_scale  = 1\n",
    "ecal_scale  = 0.2\n",
    "pt_scale    = 0.02\n",
    "dz_scale    = 10\n",
    "m0_scale    = 14\n",
    "m0_min      = 3.6\n",
    "m0_min_un   = 1.2\n",
    "m0_max      = 14\n",
    "mtrue_bins = np.arange(m0_min, m0_max, .4)\n",
    "mae_min  =0\n",
    "mae_max  = 10.5\n",
    "mae_bins = np.arange(mae_min, mae_max, .5)\n",
    "mre_bins = np.arange(0, 1.05, .05)\n",
    "mass_bins = np.arange(3600,14000,400)/1000.\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "n_train = ( 1000 // BATCH_SIZE ) * BATCH_SIZE\n",
    "n_val = (500// BATCH_SIZE ) * BATCH_SIZE\n",
    "# n_train = ( 5296937 // BATCH_SIZE ) * BATCH_SIZE\n",
    "# n_val = ( 597421// BATCH_SIZE ) * BATCH_SIZE\n",
    "\n",
    "channel_list = [\"Tracks_pt\", \"Tracks_dZSig\", \"Tracks_d0Sig\", \"ECAL_energy\",\n",
    "\"HBHE_energy\", \"Pix_1\", \"Pix_2\", \"Pix_3\", \"Pix_4\", \"Tib_1\", \"Tib_2\",\n",
    "\"Tib_3\", \"Tib_4\", \"Tob_1\", \"Tob_2\", \"Tob_3\", \"Tob_4\", \"Tob_5\",\n",
    "\"Tob_6\", \"Tid_1\", \"Tec_1\", \"Tec_2\", \"Tec_3\"]\n",
    "\n",
    "indices = [0,1,2,3,4,5,6,7,8,9,10,11,12]  # channel selected for training change it to class defination too\n",
    "channels_used = [channel_list[ch] for ch in indices]\n",
    "layers_names = '_'.join(channels_used)\n",
    "\n",
    "resnet = networks.ResNet(len(indices), resblocks, [8, 16, 32, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c97034d-8987-48fb-a351-38d7a381309f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!ls /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6559ccb7-6745-42d4-93ef-f704c1fdf381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_decays = glob.glob('/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_train/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To3p6_dataset_2_unbaised_unphysical_0007_train.parquet')\n",
    "\n",
    "dset_train = ConcatDataset([ParquetDataset('%s'%d,i) for i,d in enumerate(train_decays)])\n",
    "\n",
    "\n",
    "idxs_train_t = np.random.permutation(len(dset_train))\n",
    "\n",
    "idxs_train = np.random.permutation(n_train)\n",
    "\n",
    "\n",
    "# Train dataset\n",
    "train_sampler = RandomSampler(dset_train, replacement=True, num_samples=n_train)\n",
    "train_loader  = DataLoader(dataset=dset_train, batch_size=BATCH_SIZE, num_workers=1, pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8deb002-ff2c-462e-bbc8-e0d60f33a07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resnet.to(device)\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=lr_init)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_factor, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3041390-af40-45ef-82bc-43cb92c1685d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zs(data):\n",
    "    data[:, 0, :, :] = pt_scale * data[:, 0, :, :]   #Track pT\n",
    "    data[:, 1, :, :] = dz_scale   * data[:, 1, :, :] #Track dZ\n",
    "    data[:, 2, :, :] = dz_scale   * data[:, 2, :, :] #Track d0\n",
    "    data[:, 3, :, :] = ecal_scale * data[:, 3, :, :] #ECAL\n",
    "    data[:, 4, :, :] = hcal_scale * data[:, 4, :, :] #HCAL\n",
    "    # Preprocessing\n",
    "    #data_dict['X_jet'] = data_dict['X_jet'][:, 20:105, 20:105]\n",
    "    # High Value Suppressuib\n",
    "    data[:, 1, :, :][data[:, 1, :, :] < -20] = 0\n",
    "    data[:, 1, :, :][data[:, 1, :, :] >  20] = 0\n",
    "    data[:, 2, :, :][data[:, 2, :, :] < -10] = 0\n",
    "    data[:, 2, :, :][data[:, 2, :, :] >  10] = 0\n",
    "    # Zero-Suppression\n",
    "    data[:, 0, :, :][data[:, 0, :, :] < 1.e-3] = 0.\n",
    "    # data['X_jet'][1][data['X_jet'][1] < 1.e-4] = 0.\n",
    "    # data['X_jet'][2][data['X_jet'][2] < 1.e-4] = 0.\n",
    "    data[:, 3, :, :][data[:, 3, :, :] < 1.e-3] = 0.\n",
    "    data[:, 4, :, :][data[:, 4, :, :] < 1.e-3] = 0.\n",
    "\n",
    "    data = torch.index_select(data, 1, torch.tensor(indices).to(device))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49dd8629-a380-40e4-ac62-73ba77b79fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am device   cuda:0\n",
      " am shape   torch.Size([256, 1])\n",
      "iphi device   cuda:0\n",
      " iphi shape   torch.Size([256, 1])\n",
      "ieta device   cuda:0\n",
      " ieta shape   torch.Size([256, 1])\n",
      "1: (0/3) m_pred: [2.71225405 2.63376641 2.60041237 2.70812058 2.60632753]...\n",
      "1: (0/3) m_true: [1.88051677 3.24924231 3.15504909 3.06960058 3.47096062]...\n",
      "1: (0/3) Train loss:0.045145, mae:0.632029, mre:0.329382\n",
      "am device   cuda:0\n",
      " am shape   torch.Size([256, 1])\n",
      "iphi device   cuda:0\n",
      " iphi shape   torch.Size([256, 1])\n",
      "ieta device   cuda:0\n",
      " ieta shape   torch.Size([256, 1])\n",
      "am device   cuda:0\n",
      " am shape   torch.Size([256, 1])\n",
      "iphi device   cuda:0\n",
      " iphi shape   torch.Size([256, 1])\n",
      "ieta device   cuda:0\n",
      " ieta shape   torch.Size([256, 1])\n",
      "1: Train time:43.48s in 3 steps for N:3, wgt: 768\n",
      "1: Train loss:0.062311, mae:0.872360, mre:0.503203\n"
     ]
    }
   ],
   "source": [
    "run_logger = False\n",
    "print_step = 10\n",
    "mae_best = 1\n",
    "for e in range(1):\n",
    "    epoch = e+1+load_epoch\n",
    "    epoch_wgt = 0.\n",
    "    n_trained = 0\n",
    "    resnet.train()\n",
    "    now = time.time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # X =  data['X_jet'].clone().detach().to(device).requires_grad_(True)\n",
    "        X =  data['X_jet'].to(device)\n",
    "        # print(\"X device----  \",X.device)\n",
    "        # print(\" X shape---  \",X.shape)\n",
    "        # print(\" X is_leaf---  \",X.is_leaf)\n",
    "        # print(\" X[:,0,:,:] shape  \",X[:,0,:,:].shape)\n",
    "        # print(\" X[:,0,:,:] device  \",X[:,0,:,:].device)\n",
    "        with torch.no_grad():\n",
    "             # X[:, 0, :, :] = pt_scale * X[:, 0, :, :]   #Track pT\n",
    "             # X[:, 1, :, :] = dz_scale   * X[:, 1, :, :] #Track dZ\n",
    "             # X[:, 2, :, :] = dz_scale   * X[:, 2, :, :] #Track d0\n",
    "             # X[:, 3, :, :] = ecal_scale * X[:, 3, :, :] #ECAL\n",
    "             # X[:, 4, :, :] = hcal_scale * X[:, 4, :, :] #HCAL\n",
    "             # # Preprocessing\n",
    "             # #X_dict['X_jet'] = X_dict['X_jet'][:, 20:105, 20:105]\n",
    "             # # High Value Suppressuib\n",
    "             # X[:, 1, :, :][X[:, 1, :, :] < -20] = 0\n",
    "             # X[:, 1, :, :][X[:, 1, :, :] >  20] = 0\n",
    "             # X[:, 2, :, :][X[:, 2, :, :] < -10] = 0\n",
    "             # X[:, 2, :, :][X[:, 2, :, :] >  10] = 0\n",
    "             # # Zero-Suppression\n",
    "             # X[:, 0, :, :][X[:, 0, :, :] < 1.e-3] = 0.\n",
    "             # # X['X_jet'][1][X['X_jet'][1] < 1.e-4] = 0.\n",
    "             # # X['X_jet'][2][X['X_jet'][2] < 1.e-4] = 0.\n",
    "             # X[:, 3, :, :][X[:, 3, :, :] < 1.e-3] = 0.\n",
    "             # X[:, 4, :, :][X[:, 4, :, :] < 1.e-3] = 0.\n",
    "             # print(\"---------------------------------\")\n",
    "             # print(\" X[:,0,:,:] shape  \",X[:,0,:,:].shape)\n",
    "             # print(\" X[:,0,:,:] device  \",X[:,0,:,:].device)\n",
    "             # print(\" X[:,1,:,:] device  \",X[:,1,:,:].device)\n",
    "             # print(\" X[:,2,:,:] device  \",X[:,2,:,:].device)\n",
    "             # print(\" X[:,3,:,:] device  \",X[:,3,:,:].device)\n",
    "             # print(\" X[:,4,:,:] device  \",X[:,4,:,:].device)\n",
    "             # print(\" X[:,5,:,:] device  \",X[:,5,:,:].device)\n",
    "             # print(\" X[:,6,:,:] device  \",X[:,6,:,:].device)\n",
    "             # print(\" X[:,7,:,:] device  \",X[:,7,:,:].device)\n",
    "             # print(\" X[:,8,:,:] device  \",X[:,8,:,:].device)\n",
    "             # print(\" X[:,9,:,:] device  \",X[:,9,:,:].device)\n",
    "             # print(\" X[:,10,:,:] device  \",X[:,10,:,:].device)\n",
    "             # print(\" X[:,11,:,:] device  \",X[:,11,:,:].device)\n",
    "             # print(\" X[:,12,:,:] device  \",X[:,12,:,:].device)\n",
    "             # X = torch.index_select(X, 1, torch.tensor(indices).to(device))\n",
    "             # print(\" X1 device  \",X.device)\n",
    "             # print(\" X1 shape  \",X.shape)\n",
    "             X= zs(X)\n",
    "        am = data['am'].to(device)\n",
    "        print(\"am device  \",am.device)\n",
    "        print(\" am shape  \",am.shape)\n",
    "        iphi = data['iphi'].to(device)\n",
    "        print(\"iphi device  \",iphi.device)\n",
    "        print(\" iphi shape  \",iphi.shape)\n",
    "        ieta = data['ieta'].to(device)\n",
    "        print(\"ieta device  \",ieta.device)\n",
    "        print(\" ieta shape  \",ieta.shape)\n",
    "        optimizer.zero_grad()\n",
    "        logits = resnet([X, iphi, ieta])\n",
    "        loss = mae_loss_wgtd(logits, am)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_wgt += len(am)\n",
    "        n_trained += 1\n",
    "        if i % print_step == 0:\n",
    "            logits, am = inv_transform(logits), inv_transform(am)\n",
    "            mae =  (logits-am).abs().mean()\n",
    "            mre = (((logits-am).abs())/am).mean()\n",
    "            print('%d: (%d/%d) m_pred: %s...'%(epoch, i, len(train_loader), str(np.squeeze(logits.tolist()[:5]))))\n",
    "            print('%d: (%d/%d) m_true: %s...'%(epoch, i, len(train_loader), str(np.squeeze(am.tolist()[:5]))))\n",
    "            print('%d: (%d/%d) Train loss:%f, mae:%f, mre:%f'%(epoch, i, len(train_loader), loss.item(), mae.item(), mre.item() ))\n",
    "\n",
    "    now = time.time() - now\n",
    "    logits, am = inv_transform(logits), inv_transform(am)\n",
    "    mae = (logits-am).abs().mean()\n",
    "    mre = ((logits-am).abs()/am).mean()\n",
    "    print('%d: Train time:%.2fs in %d steps for N:%d, wgt: %.f'%(epoch, now, len(train_loader), n_trained, epoch_wgt))\n",
    "    print('%d: Train loss:%f, mae:%f, mre:%f'%(epoch, loss.item(), mae.item(), mre.item() ))\n",
    "\n",
    "#     # Run Validation\n",
    "#     resnet.eval()\n",
    "#     _ = do_eval(resnet, val_loader, mae_best, epoch, 'val_pseudoscalar')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f0940-0820-47fe-89e6-8bebbbc76c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch_VEN",
   "language": "python",
   "name": "pytorch_ven"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
