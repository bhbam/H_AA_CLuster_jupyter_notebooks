{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0736fa2-0a22-4fa7-be7a-39a9820c2b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import time\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import mplhep as hep\n",
    "plt.style.use([hep.style.ROOT, hep.style.firamath])\n",
    "#from skimage.transform import rescale\n",
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "from torch.utils.data import *\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e57545-01d0-48f8-a562-87637bb2aa53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcal_scale  = 1\n",
    "ecal_scale  = 0.2\n",
    "pt_scale    = 0.02\n",
    "dz_scale    = 10\n",
    "m0_scale    = 14\n",
    "def transform_y(y):\n",
    "    return y/m0_scale\n",
    "\n",
    "def inv_transform(y):\n",
    "    return y*m0_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b08af8-d914-4155-b9c4-3ecc66986bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, filename, label):\n",
    "        self.parquet = pq.ParquetFile(filename)\n",
    "        #self.cols = None # read all columns\n",
    "        #self.cols = ['X_jet.list.item.list.item.list.item','am','apt','iphi','ieta']\n",
    "        self.cols = ['X_jet.list.item.list.item.list.item','am','iphi','ieta']\n",
    "        self.label = label\n",
    "    def __getitem__(self, index):\n",
    "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "        data['X_jet'] = np.float32(data['X_jet'][0])\n",
    "        data['X_jet'][0] = pt_scale   * data['X_jet'][0] #Track pT\n",
    "        data['X_jet'][1] = dz_scale   * data['X_jet'][1] #Track dZ\n",
    "        data['X_jet'][2] = dz_scale   * data['X_jet'][2] #Track d0\n",
    "        data['X_jet'][3] = ecal_scale * data['X_jet'][3] #ECAL\n",
    "        data['X_jet'][4] = hcal_scale * data['X_jet'][4] #HCAL\n",
    "        #data['X_jet'] = np.float32(data['X_jet'][0])/ecal_scale\n",
    "        data['am'] = transform_y(np.float32(data['am']))\n",
    "        #data['apt'] = np.float32(data['apt'])\n",
    "        data['iphi'] = np.float32(data['iphi'])/360.\n",
    "        data['ieta'] = np.float32(data['ieta'])/140.\n",
    "        data['label'] = self.label\n",
    "        # Preprocessing\n",
    "        #data_dict['X_jet'] = data_dict['X_jet'][:, 20:105, 20:105]\n",
    "        # High Value Suppressuib\n",
    "        data['X_jet'][1][data['X_jet'][1] < -20] = 0\n",
    "        data['X_jet'][1][data['X_jet'][1] >  20] = 0\n",
    "        data['X_jet'][2][data['X_jet'][2] < -10] = 0\n",
    "        data['X_jet'][2][data['X_jet'][2] >  10] = 0\n",
    "        # Zero-Suppression\n",
    "        data['X_jet'][0][data['X_jet'][0] < 1.e-3] = 0.\n",
    "        # data['X_jet'][1][data['X_jet'][1] < 1.e-4] = 0.\n",
    "        # data['X_jet'][2][data['X_jet'][2] < 1.e-4] = 0.\n",
    "        data['X_jet'][3][data['X_jet'][3] < 1.e-3] = 0.\n",
    "        data['X_jet'][4][data['X_jet'][4] < 1.e-3] = 0.\n",
    "        data['label']=self.label\n",
    "        indices = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "        newdata = [data['X_jet'][index,:,:] for index in indices]\n",
    "        data['X_jet'] = np.reshape(newdata, (len(indices),125,125))\n",
    "        return dict(data)\n",
    "    def __len__(self):\n",
    "        return self.parquet.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c54bfb-8dfb-4f71-bb63-0c7dd0f4c314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def label(mass):\n",
    "    mass_ = {'/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_train/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To3p6_dataset_2_unbaised_unphysical_0003_train.parquet':-1,'/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_valid/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To3p6_dataset_2_unbaised_unphysical_0009_train.parquet':-2}.get(mass, None)\n",
    "    return mass_\n",
    "train_decays = glob.glob('/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_train/*unphysical_0003_train*.parquet*')\n",
    "dset_train = ConcatDataset([ParquetDataset('%s'%d,label(d)) for i,d in enumerate(train_decays)])\n",
    "val_decays = glob.glob('/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_valid/*unphysical_0009_train*.parquet*')\n",
    "dset_val = ConcatDataset([ParquetDataset('%s'%d, label(d)) for i,d in enumerate(val_decays)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d476750-6b0d-4c8d-8c11-6da4416cd7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1808e3c-b348-4a59-bd0d-110f19d11dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_train=20\n",
    "n_val =10\n",
    "BATCH_SIZE=5\n",
    "idxs_train_t = np.random.permutation(len(dset_train))\n",
    "idxs_val_t   = np.random.permutation(len(dset_val))\n",
    "idxs_train = np.random.permutation(n_train)\n",
    "idxs_val   = np.random.permutation(n_val)\n",
    "train_sampler = RandomSampler(dset_train, replacement=True, num_samples=n_train)\n",
    "train_loader  = DataLoader(dataset=dset_train, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "# Val dataset\n",
    "val_sampler   = RandomSampler(dset_val, replacement=True, num_samples=n_val)\n",
    "val_loader    = DataLoader(dataset=dset_val, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c9bd30a-7c48-4f41-948d-7cadfad72945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[0.2122],\n",
      "        [0.0893],\n",
      "        [0.2369],\n",
      "        [0.2387],\n",
      "        [0.0896]]) tensor([-2, -2, -2, -2, -2])\n",
      "1 tensor([[0.0935],\n",
      "        [0.2139],\n",
      "        [0.1979],\n",
      "        [0.1592],\n",
      "        [0.1562]]) tensor([-2, -2, -2, -2, -2])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(val_loader):\n",
    "    am , label = data['am'], data['label']\n",
    "    print(i, am, label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22afa764-c811-4cbd-809d-48143593e724",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[0.1353],\n",
      "        [0.2219],\n",
      "        [0.2427],\n",
      "        [0.0944],\n",
      "        [0.0888]]) tensor([-1, -1, -1, -1, -1])\n",
      "1 tensor([[0.2231],\n",
      "        [0.1373],\n",
      "        [0.1843],\n",
      "        [0.1702],\n",
      "        [0.1983]]) tensor([-1, -1, -1, -1, -1])\n",
      "2 tensor([[0.0978],\n",
      "        [0.2438],\n",
      "        [0.1766],\n",
      "        [0.1856],\n",
      "        [0.1562]]) tensor([-1, -1, -1, -1, -1])\n",
      "3 tensor([[0.2316],\n",
      "        [0.2253],\n",
      "        [0.1387],\n",
      "        [0.2538],\n",
      "        [0.1307]]) tensor([-1, -1, -1, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    am , label = data['am'], data['label']\n",
    "    print(i, am, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41360ccf-3107-4d58-9d37-af377772a8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_decays = glob.glob('/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_train/*unphysical_0003_train*.parquet*')\n",
    "dset_train = ConcatDataset([ParquetDataset('%s'%d,i) for i,d in enumerate(train_decays)])\n",
    "val_decays = glob.glob('/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_valid/*unphysical_0009_train*.parquet*')\n",
    "dset_val = ConcatDataset([ParquetDataset('%s'%d, i) for i,d in enumerate(val_decays)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d3ba71a-07a3-4783-b755-763214466e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=20\n",
    "n_val =5\n",
    "BATCH_SIZE=2\n",
    "idxs_train_t = np.random.permutation(len(dset_train))\n",
    "idxs_val_t   = np.random.permutation(len(dset_val))\n",
    "idxs_train = np.random.permutation(n_train)\n",
    "idxs_val   = np.random.permutation(n_val)\n",
    "train_sampler = RandomSampler(dset_train, replacement=True, num_samples=n_train)\n",
    "train_loader  = DataLoader(dataset=dset_train, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "# Val dataset\n",
    "val_sampler   = RandomSampler(dset_val, replacement=True, num_samples=n_val)\n",
    "val_loader    = DataLoader(dataset=dset_val, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d30b748a-8278-4dc2-aae0-45e7ff5f2bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 2.,  ..., 1., 0., 1.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 3., 2., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]) tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(val_loader):\n",
    "    am , label = data['X_jet'], data['label']\n",
    "    print(i, am, label)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2374fa-0f1b-4d84-8de5-ab596e9d47ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, filename, label):\n",
    "        self.parquet = pq.ParquetFile(filename)\n",
    "        #self.cols = None # read all columns\n",
    "        #self.cols = ['X_jet.list.item.list.item.list.item','am','apt','iphi','ieta']\n",
    "        self.cols = ['X_jet.list.item.list.item.list.item','am','iphi','ieta']\n",
    "        self.label = label\n",
    "    def __getitem__(self, index):\n",
    "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "        return dict(data)\n",
    "    def __len__(self):\n",
    "        return self.parquet.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02866bbb-cb26-4cd8-9701-05094d0a9da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "val_decays = glob.glob('/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_valid/*unphysical_0009_train*.parquet*')\n",
    "dset_val = ConcatDataset([ParquetDataset('%s'%d, i) for i,d in enumerate(val_decays)])\n",
    "n_val =5\n",
    "BATCH_SIZE=2\n",
    "# idxs_train_t = np.random.permutation(len(dset_train))\n",
    "idxs_val_t   = np.random.permutation(len(dset_val))\n",
    "# idxs_train = np.random.permutation(n_train)\n",
    "idxs_val   = np.random.permutation(n_val)\n",
    "# Val dataset\n",
    "val_sampler   = RandomSampler(dset_val, replacement=True, num_samples=n_val)\n",
    "val_loader    = DataLoader(dataset=dset_val, batch_size=BATCH_SIZE, num_workers=1, pin_memory=True, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a171bc9-27e6-488f-a270-ebf23062bb9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc48219-41a4-4482-bdb5-d2aeab475592",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/global/homes/b/bbbam/.conda/envs/Pytorch_VEN/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/global/homes/b/bbbam/.conda/envs/Pytorch_VEN/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/global/homes/b/bbbam/.conda/envs/Pytorch_VEN/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 358, in reduce_storage\n",
      "    fd, size = storage._share_fd_cpu_()\n",
      "RuntimeError: unable to mmap 16 bytes from file </torch_324830_602622697_63629>: Cannot allocate memory (12)\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/homes/b/bbbam/.conda/envs/Pytorch_VEN/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/global/homes/b/bbbam/.conda/envs/Pytorch_VEN/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/global/homes/b/bbbam/.conda/envs/Pytorch_VEN/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 358, in reduce_storage\n",
      "    fd, size = storage._share_fd_cpu_()\n",
      "RuntimeError: unable to mmap 16 bytes from file </torch_324830_3062887380_127206>: Cannot allocate memory (12)\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(val_loader):\n",
    "    x , label = np.float32(data['X_jet'][0]), transform_y(np.float32(data['am']))\n",
    "    print(i, x, label)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35130e5f-28d0-4fab-85ff-2267a9eae9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:, 0] *= pt_scale\n",
    "data[:, 1] *= dz_scale\n",
    "data[:, 2] *= dz_scale\n",
    "data[:, 3] *= ecal_scale\n",
    "data[:, 4] *= hcal_scale\n",
    "\n",
    "# Transform am, iphi, and ieta\n",
    "target = transform_y(target)  # Assuming transform_y is a PyTorch compatible function\n",
    "iphi /= 360.\n",
    "ieta /= 140.\n",
    "\n",
    "# High Value Suppression\n",
    "data[:, 1].clamp_(-20, 20)  # Clamps values to be between -20 and 20\n",
    "data[:, 2].clamp_(-10, 10)\n",
    "\n",
    "# Zero-Suppression\n",
    "# data[data < 1.e-3] = 0.\n",
    "data[:, 0][data[:, 0]<1.e-3] = 0.\n",
    "data[:, 3][data[:, 3]<1.e-3] = 0.\n",
    "data[:, 4][data[:, 4]<1.e-3] = 0.\n",
    "\n",
    "indices = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "data = data[:, indices, :, :].reshape(len(indices), 125, 125)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch_VEN",
   "language": "python",
   "name": "pytorch_ven"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
