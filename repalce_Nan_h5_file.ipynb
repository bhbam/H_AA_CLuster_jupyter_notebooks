{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "317b3ac5-2148-497e-8456-4ed01f14c621",
   "metadata": {
    "id": "317b3ac5-2148-497e-8456-4ed01f14c621",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import shutil\n",
    "import random\n",
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import mplhep as hep\n",
    "plt.style.use([hep.style.ROOT, hep.style.firamath])\n",
    "minimum_nonzero_pixels = 3\n",
    "\n",
    "def estimate_population_parameters(all_sample_sizes, all_sample_means, all_sample_stds):\n",
    "    population_means = []\n",
    "    population_stds = []\n",
    "    for j in range(len(all_sample_means)):\n",
    "        sample_means = all_sample_means[j]\n",
    "        sample_stds = all_sample_stds[j]\n",
    "        sample_sizes = all_sample_sizes[j]\n",
    "        sample_means = sample_means[sample_sizes != 0]\n",
    "        sample_stds = sample_stds[sample_sizes != 0]\n",
    "        sample_sizes = sample_sizes[sample_sizes != 0]\n",
    "        weighted_sum_of_variances = sum((n - 1) * s**2 for n, s in zip(sample_sizes, sample_stds))\n",
    "        total_degrees_of_freedom = sum(n - 1 for n in sample_sizes)\n",
    "        combined_variance = weighted_sum_of_variances / total_degrees_of_freedom\n",
    "        population_std = np.sqrt(combined_variance)\n",
    "        weighted_sum_of_means = sum(n * mean for n, mean in zip(sample_sizes, sample_means))\n",
    "        total_observations = sum(sample_sizes)\n",
    "        population_mean = weighted_sum_of_means / total_observations\n",
    "        population_stds.append(population_std)\n",
    "        population_means.append(population_mean)\n",
    "\n",
    "    return population_means, population_stds\n",
    "\n",
    "def alphanum_key(s):\n",
    "    \"\"\" Turn a string into a list of string and number chunks.\n",
    "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
    "    \"\"\"\n",
    "    return [int(c) if c.isdigit() else c for c in re.split('([0-9]+)',s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59039e9b-833a-46b4-bc60-38f41a1afcb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after outlier mean  :  [ 1.95973739 -0.91428634  0.41695268  0.4351373   0.02550794  1.03056946\n",
      "  1.02679871  1.03097382  1.03844135  1.62629992  1.6815035   1.68042818\n",
      "  1.68519924] \n",
      "\n",
      "after outlier std  :  [2.64603079e+01 2.85947850e+02 2.78975093e+01 2.07958377e+00\n",
      " 8.02803342e-02 1.82661149e-01 1.69144090e-01 1.82877912e-01\n",
      " 2.07325558e-01 9.95635728e-01 1.09017309e+00 1.07802985e+00\n",
      " 1.12664562e+00] \n",
      "\n",
      "total selected jets :  [426826 416817 425975 416835 416735 425962 435157 416918 416843 428532] \n",
      "\n",
      "Nan repalced by:    [-7.40632876e-02  3.19738840e-03 -1.49458747e-02 -2.09242499e-01\n",
      " -3.17735883e-01 -5.64197402e+00 -6.07055627e+00 -5.63749773e+00\n",
      " -5.00874738e+00 -1.63342865e+00 -1.54241883e+00 -1.55879560e+00\n",
      " -1.49576691e+00] \n",
      "\n",
      "(13, 125, 125)\n"
     ]
    }
   ],
   "source": [
    "def combined_mean_std(size, mean, std):\n",
    "    mean_ = np.dot(size, mean)/np.sum(size)\n",
    "    std_ = np.sqrt((np.dot((np.array(size)-1), np.square(std)) + np.dot(size,np.square(mean-mean_)))/(np.sum(size)-1))\n",
    "    return mean_, std_\n",
    "\n",
    "# mean_ = []\n",
    "# std_ = []\n",
    "# size_ = []\n",
    "# file_path = np.sort(glob.glob(\"mean_std_record_original_dataset/*\"))\n",
    "# for file in file_path:\n",
    "#     with open(file, 'r') as file:\n",
    "#         data = json.load(file)\n",
    "#     mean_.append(data['original_mean'])\n",
    "#     std_.append(data['original_std'])\n",
    "#     size_.append(data['number_of_jets'])\n",
    "# mean = np.array(mean_)\n",
    "# std = np.array(std_)\n",
    "# orig_size = np.array(size_)\n",
    "\n",
    "\n",
    "# orig_mean, orig_std = combined_mean_std(orig_size, mean, std)\n",
    "# print(\"original mean  :\" , orig_mean,\"\\n\")\n",
    "# print(\"original std  :\" , orig_std,\"\\n\")\n",
    "# print(\"totoal samples  :\" , orign_size,\"\\n\")\n",
    "\n",
    "\n",
    "### Calculate combined mean and std for data after outlier Run this before converting to normalised h5-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "mean_ = []\n",
    "std_ = []\n",
    "size_ = []\n",
    "file_path = np.sort(glob.glob(\"mean_std_record_after_outlier/*\"))\n",
    "for file in file_path:\n",
    "    with open(file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    mean_.append(data['after_outlier_mean'])\n",
    "    std_.append(data['after_outlier_std'])\n",
    "    size_.append(data['number_of_selected_jets'])\n",
    "mean = np.array(mean_)\n",
    "std = np.array(std_)\n",
    "size = np.array(size_)\n",
    "\n",
    "\n",
    "after_outlier_mean, after_outlier_std = combined_mean_std(size, mean, std)\n",
    "nan_replace = - after_outlier_mean/after_outlier_std\n",
    "\n",
    "dim = (125, 125)\n",
    "\n",
    "# Generate the desired array\n",
    "nan_replace_array = np.array([np.full(dim, v) for v in nan_replace])\n",
    "\n",
    "print(\"after outlier mean  : \" , after_outlier_mean,\"\\n\")\n",
    "print(\"after outlier std  : \" , after_outlier_std,\"\\n\")\n",
    "print(\"total selected jets : \" , size, \"\\n\")\n",
    "print(\"Nan repalced by:   \",nan_replace, \"\\n\")\n",
    "print(nan_replace_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f80892-e7ac-4d0f-a799-8de9906e78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# file = '/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_normalised_combined.hd5'\n",
    "# data = h5py.File(f'{file}', 'r')\n",
    "# num_images = data[\"all_jet\"].shape[0]\n",
    "# # num_images = 5000  # Adjusted number of images for processing\n",
    "# batch_size = 4000\n",
    "\n",
    "# print(f\"Processing file ---> {file}\\n\")\n",
    "\n",
    "# outdir = '/pscratch/sd/b/bbbam/'\n",
    "# outfile = 'IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_normalized_NAN_removed_train.h5'\n",
    "\n",
    "# with h5py.File(f'{outdir}/{outfile}', 'w') as proper_data:\n",
    "#     dataset_names = ['all_jet', 'am', 'ieta', 'iphi', 'm0']\n",
    "#     datasets = {\n",
    "#         name: proper_data.create_dataset(\n",
    "#             name,\n",
    "#             (num_images, 13, 125, 125) if 'jet' in name else (num_images, 1),\n",
    "#             dtype='float32',\n",
    "#             compression='lzf',\n",
    "#             chunks=(batch_size, 13, 125, 125) if 'jet' in name else (batch_size, 1),\n",
    "#         ) for name in dataset_names\n",
    "#     }\n",
    "\n",
    "#     for start_idx in tqdm(range(0, num_images, batch_size)):\n",
    "#         end_idx = min(start_idx + batch_size, num_images)\n",
    "#         images_batch = data[\"all_jet\"][start_idx:end_idx, :, :, :]\n",
    "#         am_batch = data[\"am\"][start_idx:end_idx, :]\n",
    "#         ieta_batch = data[\"ieta\"][start_idx:end_idx, :]\n",
    "#         iphi_batch = data[\"iphi\"][start_idx:end_idx, :]\n",
    "#         m0_batch = data[\"m0\"][start_idx:end_idx, :]\n",
    "\n",
    "#         # Replace NaN values in images_batch with the specified transformation\n",
    "#         nan_mask = np.isnan(images_batch)\n",
    "#         images_batch[nan_mask] =  np.tile(nan_replace_array, (end_idx-start_idx, 1, 1, 1))[nan_mask]\n",
    "#         # Write the processed batch to the new HDF5 file\n",
    "#         proper_data['all_jet'][start_idx:end_idx, :, :, :] = images_batch\n",
    "#         proper_data['am'][start_idx:end_idx, :] = am_batch\n",
    "#         proper_data['ieta'][start_idx:end_idx, :] = ieta_batch\n",
    "#         proper_data['iphi'][start_idx:end_idx, :] = iphi_batch\n",
    "#         proper_data['m0'][start_idx:end_idx, :] = m0_batch\n",
    "# data.close()\n",
    "# print(\">>>>>>>>>>>>>>> DONE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff9ed02-83cc-47b2-8ee6-2eba316ecf12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file = '/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_normalised_combined.hd5'\n",
    "# out_ = (file.split('/')[-1]).split('.')[:-1][0]\n",
    "# out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b322b7f8-e857-4717-8f1a-9c64ebea1888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repalce_NAN(file_, nan_replace_array): \n",
    "    file = file_\n",
    "    data = h5py.File(f'{file}', 'r')\n",
    "    num_images = data[\"all_jet\"].shape[0]\n",
    "    # num_images = 5000  # Adjusted number of images for processing\n",
    "    batch_size = 4000\n",
    "\n",
    "    print(f\"Processing file ---> {file}\\n\")\n",
    "    tag = 'NAN_removed'\n",
    "    outdir = '/pscratch/sd/b/bbbam/normalized_nan_replaced_h5'\n",
    "    if not os.path.exists(outdir):\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(outdir)\n",
    "    out_prefix = (file.split('/')[-1]).split('.')[:-1][0]\n",
    "    outfile = f'{out_prefix}_{tag}_train.h5'\n",
    "\n",
    "    with h5py.File(f'{outdir}/{outfile}', 'w') as proper_data:\n",
    "        dataset_names = ['all_jet', 'am', 'ieta', 'iphi', 'm0']\n",
    "        datasets = {\n",
    "            name: proper_data.create_dataset(\n",
    "                name,\n",
    "                (num_images, 13, 125, 125) if 'jet' in name else (num_images, 1),\n",
    "                dtype='float32',\n",
    "                compression='lzf',\n",
    "                chunks=(batch_size, 13, 125, 125) if 'jet' in name else (batch_size, 1),\n",
    "            ) for name in dataset_names\n",
    "        }\n",
    "\n",
    "        for start_idx in tqdm(range(0, num_images, batch_size)):\n",
    "            end_idx = min(start_idx + batch_size, num_images)\n",
    "            images_batch = data[\"all_jet\"][start_idx:end_idx, :, :, :]\n",
    "            am_batch = data[\"am\"][start_idx:end_idx, :]\n",
    "            ieta_batch = data[\"ieta\"][start_idx:end_idx, :]\n",
    "            iphi_batch = data[\"iphi\"][start_idx:end_idx, :]\n",
    "            m0_batch = data[\"m0\"][start_idx:end_idx, :]\n",
    "\n",
    "            # Replace NaN values in images_batch with the specified transformation\n",
    "            nan_mask = np.isnan(images_batch)\n",
    "            images_batch[nan_mask] =  np.tile(nan_replace_array, (end_idx-start_idx, 1, 1, 1))[nan_mask]\n",
    "            # Write the processed batch to the new HDF5 file\n",
    "            proper_data['all_jet'][start_idx:end_idx, :, :, :] = images_batch\n",
    "            proper_data['am'][start_idx:end_idx, :] = am_batch\n",
    "            proper_data['ieta'][start_idx:end_idx, :] = ieta_batch\n",
    "            proper_data['iphi'][start_idx:end_idx, :] = iphi_batch\n",
    "            proper_data['m0'][start_idx:end_idx, :] = m0_batch\n",
    "    data.close()\n",
    "    print(\">>>>>>>>>>>>>>> DONE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0838b-e0ef-4638-9863-40fa939228dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after outlier mean  :  [ 1.95973739 -0.91428634  0.41695268  0.4351373   0.02550794  1.03056946\n",
      "  1.02679871  1.03097382  1.03844135  1.62629992  1.6815035   1.68042818\n",
      "  1.68519924] \n",
      "\n",
      "after outlier std  :  [2.64603079e+01 2.85947850e+02 2.78975093e+01 2.07958377e+00\n",
      " 8.02803342e-02 1.82661149e-01 1.69144090e-01 1.82877912e-01\n",
      " 2.07325558e-01 9.95635728e-01 1.09017309e+00 1.07802985e+00\n",
      " 1.12664562e+00] \n",
      "\n",
      "total selected jets :  [426826 416817 425975 416835 416735 425962 435157 416918 416843 428532] \n",
      "\n",
      "Nan repalced by:    [-7.40632876e-02  3.19738840e-03 -1.49458747e-02 -2.09242499e-01\n",
      " -3.17735883e-01 -5.64197402e+00 -6.07055627e+00 -5.63749773e+00\n",
      " -5.00874738e+00 -1.63342865e+00 -1.54241883e+00 -1.55879560e+00\n",
      " -1.49576691e+00] \n",
      "\n",
      "(13, 125, 125)\n",
      "Processing file ---> /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_normalised_train_hd5/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_0005_normalized_train.h5\n",
      "Processing file ---> /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_normalised_train_hd5/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_0002_normalized_train.h5\n",
      "Processing file ---> /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_normalised_train_hd5/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_0000_normalized_train.h5\n",
      "Processing file ---> /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_normalised_train_hd5/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_0007_normalized_train.h5\n",
      "\n",
      "Processing file ---> /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_normalised_train_hd5/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_0008_normalized_train.h5\n",
      "Processing file ---> /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_normalised_train_hd5/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_0009_normalized_train.h5\n",
      "\n",
      "Processing file ---> /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_normalised_train_hd5/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_0003_normalized_train.h5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ---> /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_normalised_train_hd5/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_0006_normalized_train.h5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ---> /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_normalised_train_hd5/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_0004_normalized_train.h5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [08:12<00:00, 27.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>> DONE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [09:03<00:00, 28.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>> DONE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [09:04<00:00, 28.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>> DONE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [09:22<00:00, 29.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>> DONE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "def combined_mean_std(size, mean, std):\n",
    "    mean_ = np.dot(size, mean)/np.sum(size)\n",
    "    std_ = np.sqrt((np.dot((np.array(size)-1), np.square(std)) + np.dot(size,np.square(mean-mean_)))/(np.sum(size)-1))\n",
    "    return mean_, std_\n",
    "\n",
    "\n",
    "mean_ = []\n",
    "std_ = []\n",
    "size_ = []\n",
    "file_path = np.sort(glob.glob(\"mean_std_record_after_outlier/*\"))\n",
    "for file in file_path:\n",
    "    with open(file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    mean_.append(data['after_outlier_mean'])\n",
    "    std_.append(data['after_outlier_std'])\n",
    "    size_.append(data['number_of_selected_jets'])\n",
    "mean = np.array(mean_)\n",
    "std = np.array(std_)\n",
    "size = np.array(size_)\n",
    "\n",
    "\n",
    "after_outlier_mean, after_outlier_std = combined_mean_std(size, mean, std)\n",
    "nan_replace = - after_outlier_mean/after_outlier_std\n",
    "\n",
    "dim = (125, 125)\n",
    "\n",
    "# Generate the desired array\n",
    "nan_replace_array = np.array([np.full(dim, v) for v in nan_replace])\n",
    "\n",
    "print(\"after outlier mean  : \" , after_outlier_mean,\"\\n\")\n",
    "print(\"after outlier std  : \" , after_outlier_std,\"\\n\")\n",
    "print(\"total selected jets : \" , size, \"\\n\")\n",
    "print(\"Nan repalced by:   \",nan_replace, \"\\n\")\n",
    "print(nan_replace_array.shape)\n",
    "\n",
    "\n",
    "### Run only once to calculate original mean and std\n",
    "def process_files(file):\n",
    "    file_path = file[0]\n",
    "    repalce_NAN(file_path, nan_replace_array)\n",
    "    \n",
    "file_list = glob.glob(\"/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_normalised_train_hd5/*\")   \n",
    "args = list(zip(file_list)) \n",
    "with Pool(10) as p:\n",
    "    p.map(process_files,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138b4c6c-9458-4442-a76f-6e8b6844029b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN.shape (2000, 13, 125, 125)\n",
      "nan:   False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN.shape (1000, 13, 125, 125)\n",
      "nan:   False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_normalised_train_hd5_test/*0000*')\n",
    "num_images = 3000\n",
    "batch_size=2000\n",
    "for file in files:\n",
    "    data = h5py.File(f'{file}', 'r')\n",
    "    for start_idx in tqdm(range(0, num_images,batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, num_images)\n",
    "        images_batch = data[\"all_jet\"][start_idx:end_idx, :, :, :]\n",
    "        am_batch = data[\"am\"][start_idx:end_idx, :]\n",
    "        nan = np.isnan(images_batch)\n",
    "        print(\"NaN.shape\", nan.shape)\n",
    "        print(\"nan:  \",np.any(nan))\n",
    "    data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c580753-72eb-45ef-a84d-85c95484e6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/global/u1/b/bbbam/H_AA_CLuster_jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570201e-1c5f-4f62-be7e-ee756b5abc56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
