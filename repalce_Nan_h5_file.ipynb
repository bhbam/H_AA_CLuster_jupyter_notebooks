{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317b3ac5-2148-497e-8456-4ed01f14c621",
   "metadata": {
    "id": "317b3ac5-2148-497e-8456-4ed01f14c621",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import shutil\n",
    "import random\n",
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import mplhep as hep\n",
    "plt.style.use([hep.style.ROOT, hep.style.firamath])\n",
    "minimum_nonzero_pixels = 3\n",
    "\n",
    "def estimate_population_parameters(all_sample_sizes, all_sample_means, all_sample_stds):\n",
    "    population_means = []\n",
    "    population_stds = []\n",
    "    for j in range(len(all_sample_means)):\n",
    "        sample_means = all_sample_means[j]\n",
    "        sample_stds = all_sample_stds[j]\n",
    "        sample_sizes = all_sample_sizes[j]\n",
    "        sample_means = sample_means[sample_sizes != 0]\n",
    "        sample_stds = sample_stds[sample_sizes != 0]\n",
    "        sample_sizes = sample_sizes[sample_sizes != 0]\n",
    "        weighted_sum_of_variances = sum((n - 1) * s**2 for n, s in zip(sample_sizes, sample_stds))\n",
    "        total_degrees_of_freedom = sum(n - 1 for n in sample_sizes)\n",
    "        combined_variance = weighted_sum_of_variances / total_degrees_of_freedom\n",
    "        population_std = np.sqrt(combined_variance)\n",
    "        weighted_sum_of_means = sum(n * mean for n, mean in zip(sample_sizes, sample_means))\n",
    "        total_observations = sum(sample_sizes)\n",
    "        population_mean = weighted_sum_of_means / total_observations\n",
    "        population_stds.append(population_std)\n",
    "        population_means.append(population_mean)\n",
    "\n",
    "    return population_means, population_stds\n",
    "\n",
    "def alphanum_key(s):\n",
    "    \"\"\" Turn a string into a list of string and number chunks.\n",
    "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
    "    \"\"\"\n",
    "    return [int(c) if c.isdigit() else c for c in re.split('([0-9]+)',s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59039e9b-833a-46b4-bc60-38f41a1afcb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after outlier mean  :  [ 1.95973739 -0.91428634  0.41695268  0.4351373   0.02550794  1.03056946\n",
      "  1.02679871  1.03097382  1.03844135  1.62629992  1.6815035   1.68042818\n",
      "  1.68519924] \n",
      "\n",
      "after outlier std  :  [2.64603079e+01 2.85947850e+02 2.78975093e+01 2.07958377e+00\n",
      " 8.02803342e-02 1.82661149e-01 1.69144090e-01 1.82877912e-01\n",
      " 2.07325558e-01 9.95635728e-01 1.09017309e+00 1.07802985e+00\n",
      " 1.12664562e+00] \n",
      "\n",
      "total selected jets :  [426826 416817 425975 416835 416735 425962 435157 416918 416843 428532] \n",
      "\n",
      "Nan repalced by:    [-7.40632876e-02  3.19738840e-03 -1.49458747e-02 -2.09242499e-01\n",
      " -3.17735883e-01 -5.64197402e+00 -6.07055627e+00 -5.63749773e+00\n",
      " -5.00874738e+00 -1.63342865e+00 -1.54241883e+00 -1.55879560e+00\n",
      " -1.49576691e+00] \n",
      "\n",
      "(13, 125, 125)\n"
     ]
    }
   ],
   "source": [
    "def combined_mean_std(size, mean, std):\n",
    "    mean_ = np.dot(size, mean)/np.sum(size)\n",
    "    std_ = np.sqrt((np.dot((np.array(size)-1), np.square(std)) + np.dot(size,np.square(mean-mean_)))/(np.sum(size)-1))\n",
    "    return mean_, std_\n",
    "\n",
    "# mean_ = []\n",
    "# std_ = []\n",
    "# size_ = []\n",
    "# file_path = np.sort(glob.glob(\"mean_std_record_original_dataset/*\"))\n",
    "# for file in file_path:\n",
    "#     with open(file, 'r') as file:\n",
    "#         data = json.load(file)\n",
    "#     mean_.append(data['original_mean'])\n",
    "#     std_.append(data['original_std'])\n",
    "#     size_.append(data['number_of_jets'])\n",
    "# mean = np.array(mean_)\n",
    "# std = np.array(std_)\n",
    "# orig_size = np.array(size_)\n",
    "\n",
    "\n",
    "# orig_mean, orig_std = combined_mean_std(orig_size, mean, std)\n",
    "# print(\"original mean  :\" , orig_mean,\"\\n\")\n",
    "# print(\"original std  :\" , orig_std,\"\\n\")\n",
    "# print(\"totoal samples  :\" , orign_size,\"\\n\")\n",
    "\n",
    "\n",
    "### Calculate combined mean and std for data after outlier Run this before converting to normalised h5-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "mean_ = []\n",
    "std_ = []\n",
    "size_ = []\n",
    "file_path = np.sort(glob.glob(\"mean_std_record_after_outlier/*\"))\n",
    "for file in file_path:\n",
    "    with open(file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    mean_.append(data['after_outlier_mean'])\n",
    "    std_.append(data['after_outlier_std'])\n",
    "    size_.append(data['number_of_selected_jets'])\n",
    "mean = np.array(mean_)\n",
    "std = np.array(std_)\n",
    "size = np.array(size_)\n",
    "\n",
    "\n",
    "after_outlier_mean, after_outlier_std = combined_mean_std(size, mean, std)\n",
    "nan_replace = - after_outlier_mean/after_outlier_std\n",
    "\n",
    "dim = (125, 125)\n",
    "\n",
    "# Generate the desired array\n",
    "nan_replace_array = np.array([np.full(dim, v) for v in nan_replace])\n",
    "\n",
    "print(\"after outlier mean  : \" , after_outlier_mean,\"\\n\")\n",
    "print(\"after outlier std  : \" , after_outlier_std,\"\\n\")\n",
    "print(\"total selected jets : \" , size, \"\\n\")\n",
    "print(\"Nan repalced by:   \",nan_replace, \"\\n\")\n",
    "print(nan_replace_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6db1403-d875-4db6-829b-587c7dc639f2",
   "metadata": {
    "id": "d6db1403-d875-4db6-829b-587c7dc639f2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ---> /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_normalised_combined.hd5\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m proper_data:\n\u001b[1;32m     11\u001b[0m     dataset_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_jet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mieta\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miphi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm0\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mproper_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m125\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m125\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify an appropriate data type\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlzf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m125\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m125\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset_names\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     22\u001b[0m     start_idx_, end_idx_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start_idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_images, batch_size)):\n",
      "Cell \u001b[0;32mIn[41], line 13\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m proper_data:\n\u001b[1;32m     11\u001b[0m     dataset_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_jet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mieta\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miphi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm0\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 13\u001b[0m         name: \u001b[43mproper_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m125\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m125\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify an appropriate data type\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlzf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m125\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m125\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m dataset_names\n\u001b[1;32m     20\u001b[0m     }\n\u001b[1;32m     22\u001b[0m     start_idx_, end_idx_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start_idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_images, batch_size)):\n",
      "File \u001b[0;32m/global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m/global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages/h5py/_hl/dataset.py:67\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m     66\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m (chunks,)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m     68\u001b[0m     chunk \u001b[38;5;241m>\u001b[39m dim \u001b[38;5;28;01mfor\u001b[39;00m dim, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tmp_shape, chunks) \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     69\u001b[0m ):\n\u001b[1;32m     70\u001b[0m     errmsg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk shape must not be greater than data shape in any dimension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m     71\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not compatible with \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(chunks, shape)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(errmsg)\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    " \n",
    "    \n",
    "file = '/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_normalised_combined.hd5'\n",
    "data = h5py.File(f'{file}', 'r')\n",
    "num_images = data[\"all_jet\"].shape[0]\n",
    "num_images = 6400\n",
    "batch_size = 3200\n",
    "print(f\"processing file ---> {file}\\n\")\n",
    "outdir = '/pscratch/sd/b/bbbam/'\n",
    "outfile = f'IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_normalized_NAN_removed_train.h5'\n",
    "\n",
    "with h5py.File(f'{outdir}/{outfile}', 'w') as proper_data:\n",
    "    dataset_names = ['all_jet', 'am', 'ieta', 'iphi', 'm0']\n",
    "    datasets = {\n",
    "        name: proper_data.create_dataset(\n",
    "            name,\n",
    "            (size,13, 125, 125) if 'jet' in name else (size, 1),\n",
    "            dtype='float32',  # Specify an appropriate data type\n",
    "            compression='lzf',\n",
    "            chunks=(batch_size, 13, 125, 125) if 'jet' in name else (1, 1),\n",
    "        ) for name in dataset_names\n",
    "    }\n",
    "\n",
    "    start_idx_, end_idx_ = 0, 0\n",
    "    for start_idx in tqdm(range(0, num_images, batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, num_images)\n",
    "        images_batch = data[\"all_jet\"][start_idx:end_idx, :, :, :]\n",
    "        am_batch = data[\"am\"][start_idx:end_idx, :]\n",
    "        ieta_batch = data[\"ieta\"][start_idx:end_idx, :]\n",
    "        iphi_batch = data[\"iphi\"][start_idx:end_idx, :]\n",
    "        m0_batch = data[\"m0\"][start_idx:end_idx, :]\n",
    "\n",
    "        # images_batch = (images_batch - after_outlier_mean.reshape(1, 13, 1, 1)) / after_outlier_std.reshape(1, 13, 1, 1)\n",
    "        images_batch[np.isnan(images_batch)] = nan_replace_array\n",
    "\n",
    "\n",
    "        start_idx_ = min(start_idx, end_idx_)\n",
    "        end_idx_   = min(start_idx_ + images_batch.shape[0], num_images)\n",
    "\n",
    "\n",
    "\n",
    "        proper_data['all_jet'][start_idx_:end_idx_,:,:,:] = images_batch\n",
    "        proper_data['am'][start_idx_:end_idx_] = am_batch\n",
    "        proper_data['ieta'][start_idx_:end_idx_] = ieta_batch\n",
    "        proper_data['iphi'][start_idx_:end_idx_] = iphi_batch\n",
    "        proper_data['m0'][start_idx_:end_idx_] = m0_batch\n",
    "\n",
    "\n",
    "print(\">>>>>>>>>>>>>>>DONE>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b322b7f8-e857-4717-8f1a-9c64ebea1888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ---> /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_normalised_combined.hd5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1446/1446 [7:03:53<00:00, 17.59s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>> DONE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "file = '/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_normalised_combined.hd5'\n",
    "data = h5py.File(f'{file}', 'r')\n",
    "num_images = data[\"all_jet\"].shape[0]\n",
    "# num_images = 5000  # Adjusted number of images for processing\n",
    "batch_size = 4000\n",
    "\n",
    "print(f\"Processing file ---> {file}\\n\")\n",
    "\n",
    "outdir = '/pscratch/sd/b/bbbam/'\n",
    "outfile = 'IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_normalized_NAN_removed_train.h5'\n",
    "\n",
    "with h5py.File(f'{outdir}/{outfile}', 'w') as proper_data:\n",
    "    dataset_names = ['all_jet', 'am', 'ieta', 'iphi', 'm0']\n",
    "    datasets = {\n",
    "        name: proper_data.create_dataset(\n",
    "            name,\n",
    "            (num_images, 13, 125, 125) if 'jet' in name else (num_images, 1),\n",
    "            dtype='float32',\n",
    "            compression='lzf',\n",
    "            chunks=(batch_size, 13, 125, 125) if 'jet' in name else (batch_size, 1),\n",
    "        ) for name in dataset_names\n",
    "    }\n",
    "\n",
    "    for start_idx in tqdm(range(0, num_images, batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, num_images)\n",
    "        images_batch = data[\"all_jet\"][start_idx:end_idx, :, :, :]\n",
    "        am_batch = data[\"am\"][start_idx:end_idx, :]\n",
    "        ieta_batch = data[\"ieta\"][start_idx:end_idx, :]\n",
    "        iphi_batch = data[\"iphi\"][start_idx:end_idx, :]\n",
    "        m0_batch = data[\"m0\"][start_idx:end_idx, :]\n",
    "\n",
    "        # Replace NaN values in images_batch with the specified transformation\n",
    "        nan_mask = np.isnan(images_batch)\n",
    "        images_batch[nan_mask] =  np.tile(nan_replace_array, (end_idx-start_idx, 1, 1, 1))[nan_mask]\n",
    "        # Write the processed batch to the new HDF5 file\n",
    "        proper_data['all_jet'][start_idx:end_idx, :, :, :] = images_batch\n",
    "        proper_data['am'][start_idx:end_idx, :] = am_batch\n",
    "        proper_data['ieta'][start_idx:end_idx, :] = ieta_batch\n",
    "        proper_data['iphi'][start_idx:end_idx, :] = iphi_batch\n",
    "        proper_data['m0'][start_idx:end_idx, :] = m0_batch\n",
    "data.close()\n",
    "print(\">>>>>>>>>>>>>>> DONE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f80892-e7ac-4d0f-a799-8de9906e78d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "138b4c6c-9458-4442-a76f-6e8b6844029b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN.shape (3200, 13, 125, 125)\n",
      "nan:   False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:09<00:00,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN.shape (1000, 13, 125, 125)\n",
      "nan:   False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file = '/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_normalized_NAN_removed_train.h5'\n",
    "data = h5py.File(f'{file}', 'r')\n",
    "for start_idx in tqdm(range(0, num_images, 4000)):\n",
    "    end_idx = min(start_idx + batch_size, num_images)\n",
    "    images_batch = data[\"all_jet\"][start_idx:end_idx, :, :, :]\n",
    "    am_batch = data[\"am\"][start_idx:end_idx, :]\n",
    "    nan = np.isnan(images_batch)\n",
    "    print(\"NaN.shape\", nan.shape)\n",
    "    print(\"nan:  \",np.any(nan))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c580753-72eb-45ef-a84d-85c95484e6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
