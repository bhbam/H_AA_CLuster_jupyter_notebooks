{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef60ab70-24bd-4a34-a76b-d05dab4a2c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import shutil\n",
    "import random\n",
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import time\n",
    "import cupy as cp\n",
    "from multiprocessing import Pool\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f807b823-0ebd-43a3-8645-64d9fdcb8aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimate_population_parameters(all_sample_sizes, all_sample_means, all_sample_stds):\n",
    "    population_means = []\n",
    "    population_stds = []\n",
    "    for j in range(len(all_sample_means)):\n",
    "        sample_means = all_sample_means[j]\n",
    "        sample_stds = all_sample_stds[j]\n",
    "        sample_sizes = all_sample_sizes[j]\n",
    "        sample_means = sample_means[sample_sizes != 0]\n",
    "        sample_stds = sample_stds[sample_sizes != 0]\n",
    "        sample_sizes = sample_sizes[sample_sizes != 0]\n",
    "        weighted_sum_of_variances = sum((n - 1) * s**2 for n, s in zip(sample_sizes, sample_stds))\n",
    "        total_degrees_of_freedom = sum(n - 1 for n in sample_sizes)\n",
    "        combined_variance = weighted_sum_of_variances / total_degrees_of_freedom\n",
    "        population_std = np.sqrt(combined_variance)\n",
    "        weighted_sum_of_means = sum(n * mean for n, mean in zip(sample_sizes, sample_means))\n",
    "        total_observations = sum(sample_sizes)\n",
    "        population_mean = weighted_sum_of_means / total_observations\n",
    "        population_stds.append(population_std)\n",
    "        population_means.append(population_mean)\n",
    "\n",
    "    return population_means, population_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72b0ee0b-8556-4f56-90d1-bb27dcdb00c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mean_std(start_index, end_index, df):\n",
    "    #df = df[start_index:end_index]\n",
    "    size_ = []\n",
    "    mean_ = []\n",
    "    std_ = []\n",
    "    \n",
    "    xj = df.columns.get_loc('X_jet')\n",
    "    \n",
    "\n",
    "    im_all = np.array(np.array(np.array(df.iloc[:, xj].tolist()).tolist()).tolist())\n",
    "   \n",
    "    \n",
    "    # hdf5_file[\"all_jet\"][start_index:end_index, :, :, :] = im\n",
    "    # print(\"        im = im_all[3,:,:,:])----------\",      im_all[3,:,:,:])\n",
    "    \n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        im = im_all[i,:,:,:]\n",
    "        im[im < 1.e-5] = 0\n",
    "        size_channel = []\n",
    "        mean_channel = []\n",
    "        std_channel = []\n",
    "        bad_channel = False\n",
    "        for j in range(13):\n",
    "            if not bad_channel:\n",
    "                image = im[j,:,:]\n",
    "                image = image[image != 0]\n",
    "                if len(image) < 2:\n",
    "                    bad_channel = True\n",
    "                    continue\n",
    "                size_channel.append(len(image))\n",
    "                mean_channel.append(image.mean())\n",
    "                std_channel.append(image.std(ddof=1))\n",
    "        if not bad_channel:\n",
    "            size_.append(size_channel)\n",
    "            mean_.append(mean_channel)\n",
    "            std_.append(std_channel)\n",
    "        if i > 9: break\n",
    "    print(\"size_. \",size_)  \n",
    "    orig_mean, orig_std = estimate_population_parameters(size_, mean_, std_)\n",
    "    \n",
    "    return orig_mean, orig_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11cc8172-1e81-4c46-b625-01edc6222054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_files(args):\n",
    "    file_path = args[0]\n",
    "    h5py_file = args[1]\n",
    "    # batch_size = 4096\n",
    "    batch_size = 10\n",
    "    \n",
    "    print(\"------Processing file------\")\n",
    "    parquet = pq.ParquetFile(file_path)\n",
    "    print(\"file ------>   \", file_path)\n",
    "    print(\"Number of row --------> \", parquet.num_row_groups)\n",
    "    total_samples = parquet.num_row_groups\n",
    "    batch_iter = parquet.iter_batches(batch_size,use_threads=True)\n",
    "\n",
    "    start_index = 0\n",
    "    bat = 0\n",
    "    for batch in batch_iter:\n",
    "        df = batch.to_pandas(use_threads=True)\n",
    "        end_index = start_index + df.shape[0]\n",
    "        print(\"File----->\",file_path , \" Batch no.\", bat, \"Data frame shape\", df.shape, \" Start idx:\", start_index, \" end idx:\", end_index)\n",
    "\n",
    "        if end_index<=total_samples:\n",
    "            #print(\"Image shape going in append\", im.shape, \" \", start_index, \" \", end_index)\n",
    "            mean_std(start_index, end_index, df)\n",
    "            start_index += df.shape[0]\n",
    "            break\n",
    "\n",
    "        # bat +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d40628bb-e9e4-4a0f-b37a-21a0020a76eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h5_file---- /pscratch/sd/b/bbbam/mean_std_from_parquet/mean_ste_number_dataset_0005.json\n",
      "----------------------------------------\n",
      "arg -------- [('/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_all/IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_0005_train.parquet', '/pscratch/sd/b/bbbam/mean_std_from_parquet/mean_ste_number_dataset_0005.json')]\n",
      "------Processing file------\n",
      "file ------>    /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_all/IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_0005_train.parquet\n",
      "Number of row -------->  428918\n",
      "File-----> /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_all/IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_0005_train.parquet  Batch no. 0 Data frame shape (10, 7)  Start idx: 0  end idx: 10\n",
      "size_.  [[56, 24, 29, 211, 7475, 224, 195, 189, 178, 608, 446, 424, 354], [30, 23, 14, 45, 3800, 97, 109, 173, 124, 363, 397, 470, 419], [32, 24, 18, 101, 3600, 64, 122, 117, 115, 482, 406, 420, 339], [42, 9, 17, 128, 5375, 103, 121, 135, 96, 359, 340, 349, 258], [28, 4, 14, 126, 5800, 96, 129, 128, 105, 364, 322, 319, 276], [42, 40, 22, 178, 5125, 125, 156, 199, 184, 520, 432, 403, 348], [53, 31, 25, 121, 5850, 174, 155, 122, 112, 295, 305, 338, 289], [65, 16, 30, 290, 8325, 202, 202, 167, 82, 647, 384, 564, 568], [57, 5, 28, 124, 7775, 253, 304, 328, 255, 785, 664, 627, 627], [37, 17, 22, 106, 4525, 108, 157, 141, 159, 558, 484, 385, 343]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n           ^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_472154/1283532522.py\", line 23, in process_files\n    mean_std(start_index, end_index, df)\n  File \"/tmp/ipykernel_472154/130073210.py\", line 40, in mean_std\n    orig_mean, orig_std = estimate_population_parameters(size_, mean_, std_)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_472154/1728264229.py\", line 11, in estimate_population_parameters\n    weighted_sum_of_variances = sum((n - 1) * s**2 for n, s in zip(sample_sizes, sample_stds))\n                                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'int' object is not iterable\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg --------\u001b[39m\u001b[38;5;124m\"\u001b[39m, args)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(\u001b[38;5;241m12\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;124m\"\u001b[39m, toc\u001b[38;5;241m-\u001b[39mtic)\n",
      "File \u001b[0;32m/global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "parquet_dir = '/pscratch/sd/b/bbbam/'\n",
    "h5_dir = '/pscratch/sd/b/bbbam/mean_std_from_parquet/'\n",
    "if not os.path.exists(h5_dir):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(h5_dir)\n",
    "\n",
    "\n",
    "signal_files = [os.path.join(parquet_dir + 'IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_all/', f) for f in os.listdir(parquet_dir + 'IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_all/')]\n",
    "\n",
    "combined_files = signal_files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputfile_list = []\n",
    "outputfile_list = []\n",
    "\n",
    "for f in combined_files:\n",
    "    opFile = 'mean_ste_number_dataset_' + f.split('_')[-2]\n",
    "\n",
    "    # print(\"opFile-----\", opFile)\n",
    "    h5_file = h5_dir+opFile+\".json\"\n",
    "    print(\"h5_file----\", h5_file)\n",
    "    #process_files(f, h5_file, batch_size)\n",
    "    inputfile_list.append(f)\n",
    "    outputfile_list.append(h5_file)\n",
    "    tic = time.time()\n",
    "    break\n",
    "\n",
    "args = list(zip(inputfile_list,outputfile_list)) \n",
    "print(\"----------------------------------------\")\n",
    "print(\"arg --------\", args)\n",
    "\n",
    "with Pool(12) as p:\n",
    "    p.map(process_files,args)\n",
    "toc = time.time()\n",
    "\n",
    "\n",
    "print(\"It took \", toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f302ba-2b26-483f-906c-19debbf1dc92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61e321-afe0-4278-9fe3-d24328c8da95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c94742-8fd3-45bd-b2fe-4a9853d11f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
