{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24281e6-3e5b-4743-82a9-20212b01dbed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import glob\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f19c2f3e-00ac-42ef-8c15-0dd6274a6ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def combine_h5_files(master_folder, dest_file):\n",
    "#     source_files = [os.path.join(master_folder, f) for f in os.listdir(master_folder) if f.endswith('.h5')]\n",
    "#     # source_files = glob.glob(f'{master_folder}/*/*')\n",
    "\n",
    "#     with h5py.File(dest_file, 'w') as h5_dest:\n",
    "#         initialized_datasets = {}\n",
    "#         for file_name in source_files:\n",
    "#             with h5py.File(file_name, 'r') as h5_source:\n",
    "#                 copy_datasets(h5_source, h5_dest, initialized_datasets)\n",
    "\n",
    "# def copy_datasets(source, dest, initialized_datasets):\n",
    "#     for name, item in source.items():\n",
    "#         if isinstance(item, h5py.Dataset):\n",
    "#             if name not in initialized_datasets:\n",
    "#                 dest.create_dataset(\n",
    "#                     name,\n",
    "#                     shape=item.shape,\n",
    "#                     dtype=item.dtype,\n",
    "#                     compression='lzf',\n",
    "#                     chunks=item.chunks\n",
    "#                 )\n",
    "#                 initialized_datasets[name] = dest[name]\n",
    "\n",
    "#             dest_dataset = initialized_datasets[name]\n",
    "#             dest_dataset.resize((dest_dataset.shape[0] + item.shape[0]), axis=0)\n",
    "#             dest_dataset[-item.shape[0]:] = item[:]\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def combine_h5_files(master_folder, dest_file, batch_size):\n",
    "    source_files = np.sort(glob.glob(f'{master_folder}/*.h5')s  # Ensure to match only .h5 files\n",
    "\n",
    "    with h5py.File(dest_file, 'w') as h5_dest:\n",
    "        initialized_datasets = {}\n",
    "        for file_name in source_files:\n",
    "            try:\n",
    "                with h5py.File(file_name, 'r') as h5_source:\n",
    "                    copy_datasets(h5_source, h5_dest, initialized_datasets, batch_size)\n",
    "                    logging.info(f\"Copied data from {file_name}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to process file {file_name}: {e}\")\n",
    "\n",
    "def copy_datasets(source, dest, initialized_datasets, batch_size):\n",
    "    for name, item in source.items():\n",
    "        if isinstance(item, h5py.Dataset):\n",
    "            if name not in initialized_datasets:\n",
    "                shape = (0,) + item.shape[1:]  # Initialize with zero rows\n",
    "                maxshape = (None,) + item.shape[1:]\n",
    "                \n",
    "                dest.create_dataset(\n",
    "                    name,\n",
    "                    shape=shape,\n",
    "                    dtype=item.dtype,\n",
    "                    compression='lzf',\n",
    "                    chunks=(batch_size,) + item.shape[1:],\n",
    "                    maxshape=maxshape\n",
    "                )\n",
    "                initialized_datasets[name] = dest[name]\n",
    "            \n",
    "            dest_dataset = initialized_datasets[name]\n",
    "            new_size = dest_dataset.shape[0] + item.shape[0]\n",
    "            dest_dataset.resize(new_size, axis=0)\n",
    "            dest_dataset[-item.shape[0]:] = item[:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af248cfb-d013-47cc-bd29-6cc1c0a34213",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Copied data from /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_normalised_valid/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_0009_normalized_train.h5\n",
      "INFO:root:Copied data from /pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_normalised_valid/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To3p6_dataset_2_unbaised_v2_0009_normalized_train.h5\n"
     ]
    }
   ],
   "source": [
    "# Specify the master folder path and the destination file\n",
    "master_folder ='/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_normalised_valid'\n",
    "dest_file = '/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_normalised_valid.hd5'\n",
    "\n",
    "combine_h5_files(master_folder, dest_file, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c6d8f3-4836-4382-bd09-1f2f5e67d28e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys-- <KeysViewHDF5 ['all_jet', 'am', 'ieta', 'iphi', 'm0']>\n",
      "all_jet shape:  (545839, 13, 125, 125)\n",
      "am shape:  (545839, 1)\n",
      "m0 shape:  (545839, 1)\n",
      "ieta shape:   (545839, 1)\n"
     ]
    }
   ],
   "source": [
    "infile_list =glob.glob('/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_normalised_valid.hd5')\n",
    "# print(infile_list)\n",
    "\n",
    "\n",
    "# data = h5py.File('/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To17p2_dataset_2_unbaised_v2_normalised_combined.hd5', 'r')\n",
    "data = h5py.File(infile_list[0], 'r')\n",
    "print(\"keys--\", data.keys())\n",
    "print(\"all_jet shape: \", data[\"all_jet\"].shape)\n",
    "print(\"am shape: \", data[\"am\"].shape)\n",
    "print(\"m0 shape: \", data[\"m0\"].shape)\n",
    "print(\"ieta shape:  \", data[\"ieta\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a0f6f51-0c41-4e54-bfcc-77095f5873de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_files = [os.path.join(master_folder, f) for f in os.listdir(master_folder) if f.endswith('.h5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96a5683a-4b30-4319-b25d-f4f5ea85d3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_normalised_valid/IMG_aToTauTau_Hadronic_tauDR0p4_m14p8To17p2_dataset_2_unbaised_v2_0009_normalized_train.h5',\n",
       " '/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_normalised_valid/IMG_aToTauTau_Hadronic_tauDR0p4_m1p2To3p6_dataset_2_unbaised_v2_0009_normalized_train.h5',\n",
       " '/pscratch/sd/b/bbbam/IMG_aToTauTau_Hadronic_tauDR0p4_normalised_valid/IMG_aToTauTau_Hadronic_tauDR0p4_m3p6To14p8_dataset_2_unbaised_v2_0009_normalized_train.h5']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d2842-b0ea-4414-9dae-ff9b51641054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
